#!/usr/bin/env python3
"""
Performance and Scalability Testing - Task 14.5

This script implements comprehensive performance and scalability testing for the 
migration system, measuring performance under various load conditions and 
identifying optimization opportunities.

Test Categories:
1. Migration Duration Measurements
2. Resource Utilization Monitoring (CPU, Memory, Disk I/O)
3. Scalability Testing with Increasing Data Volumes
4. Performance Bottleneck Identification
5. Memory Efficiency Analysis
6. Concurrent Operation Performance
7. Database Operation Benchmarks

Author: Task Master AI  
Date: 2025-01-XX
"""

import os
import sys
import json
import time
import sqlite3
import logging
import threading
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import psutil
import gc

# Add project root to Python path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Import migration system components
from data.database.migration_system.version_detection import VersionManager, DatabaseVersion
from data.database.migration_system.schema_transformation import SchemaTransformationManager
from data.database.migration_system.data_conversion import DataConversionManager
from data.database.migration_system.data_integrity import DataIntegrityManager, ValidationLevel
from data.database.connection import SQLiteConnectionManager, ConnectionConfig

class PerformanceTestType(Enum):
    """Types of performance tests"""
    MIGRATION_DURATION = "migration_duration"
    RESOURCE_UTILIZATION = "resource_utilization" 
    SCALABILITY_TESTING = "scalability_testing"
    BOTTLENECK_ANALYSIS = "bottleneck_analysis"
    MEMORY_EFFICIENCY = "memory_efficiency"
    CONCURRENT_OPERATIONS = "concurrent_operations"
    DATABASE_BENCHMARKS = "database_benchmarks"

class LoadLevel(Enum):
    """Load levels for testing"""
    LIGHT = "light"      # Small datasets
    MODERATE = "moderate" # Medium datasets
    HEAVY = "heavy"      # Large datasets
    EXTREME = "extreme"  # Very large datasets

@dataclass
class PerformanceMetrics:
    """Container for performance metrics"""
    operation_name: str
    start_time: float
    end_time: float
    duration: float
    cpu_usage_before: float
    cpu_usage_after: float
    memory_before_mb: float
    memory_after_mb: float
    memory_peak_mb: float
    disk_read_before: int
    disk_write_before: int
    disk_read_after: int
    disk_write_after: int
    records_processed: int
    throughput_records_per_sec: float
    additional_metrics: Dict[str, Any]

@dataclass
class ScalabilityTestResult:
    """Container for scalability test results"""
    dataset_size: int
    records_count: int
    processing_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    throughput: float
    performance_score: float

@dataclass
class PerformanceReport:
    """Comprehensive performance testing report"""
    session_id: str
    test_timestamp: str
    total_tests: int
    performance_metrics: List[PerformanceMetrics]
    scalability_results: List[ScalabilityTestResult]
    bottlenecks_identified: List[str]
    optimization_recommendations: List[str]
    system_specifications: Dict[str, Any]
    overall_performance_score: float

class PerformanceTestingFramework:
    """
    Comprehensive performance and scalability testing framework
    
    This framework measures migration system performance under various
    load conditions and identifies optimization opportunities.
    """
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or PROJECT_ROOT
        self.test_datasets_dir = self.project_root / "test_datasets"
        self.results_dir = self.project_root / "scripts" / "performance_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # Test session tracking
        self.session_id = f"perf_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.performance_metrics: List[PerformanceMetrics] = []
        self.scalability_results: List[ScalabilityTestResult] = []
        self.bottlenecks: List[str] = []
        
        # Setup logging
        self._setup_logging()
        
        # System monitoring
        self.process = psutil.Process()
        self.system_specs = self._gather_system_specifications()
        
    def _setup_logging(self):
        """Configure logging for performance testing"""
        log_file = self.results_dir / f"{self.session_id}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Performance testing started - Session: {self.session_id}")
    
    def _gather_system_specifications(self) -> Dict[str, Any]:
        """Gather system specifications for context"""
        try:
            return {
                "cpu_count": psutil.cpu_count(),
                "cpu_count_logical": psutil.cpu_count(logical=True),
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "disk_usage": dict(psutil.disk_usage('/')),
                "python_version": sys.version,
                "platform": sys.platform
            }
        except Exception as e:
            self.logger.warning(f"Could not gather system specs: {e}")
            return {"error": str(e)}
    
    def run_comprehensive_performance_testing(self) -> PerformanceReport:
        """
        Execute comprehensive performance testing across all categories
        
        Returns:
            PerformanceReport: Comprehensive performance results
        """
        start_time = datetime.now()
        self.logger.info("Starting comprehensive performance testing")
        
        try:
            # Test Category 1: Migration Duration Measurements
            self.logger.info("=== Testing Migration Duration ===")
            self._test_migration_duration()
            
            # Test Category 2: Resource Utilization Monitoring
            self.logger.info("=== Testing Resource Utilization ===")
            self._test_resource_utilization()
            
            # Test Category 3: Scalability Testing
            self.logger.info("=== Testing Scalability ===")
            self._test_scalability()
            
            # Test Category 4: Bottleneck Analysis
            self.logger.info("=== Analyzing Performance Bottlenecks ===")
            self._analyze_bottlenecks()
            
            # Test Category 5: Memory Efficiency Analysis
            self.logger.info("=== Testing Memory Efficiency ===")
            self._test_memory_efficiency()
            
            # Test Category 6: Concurrent Operations Performance
            self.logger.info("=== Testing Concurrent Operations ===")
            self._test_concurrent_operations()
            
            # Test Category 7: Database Operation Benchmarks
            self.logger.info("=== Running Database Benchmarks ===")
            self._test_database_benchmarks()
            
            # Generate comprehensive report
            report = self._generate_performance_report(start_time)
            
            self.logger.info("Performance testing completed successfully")
            return report
            
        except Exception as e:
            self.logger.error(f"Performance testing failed: {e}")
            raise
    
    def _test_migration_duration(self):
        """Test migration duration with different dataset sizes"""
        
        # Test with available datasets
        test_datasets = list(self.test_datasets_dir.glob("*.db"))
        
        for dataset in sorted(test_datasets):
            self.logger.info(f"Testing migration duration for {dataset.name}")
            
            try:
                metrics = self._measure_migration_operation(
                    f"migration_duration_{dataset.stem}",
                    lambda: self._simulate_migration_process(dataset)
                )
                self.performance_metrics.append(metrics)
                
            except Exception as e:
                self.logger.error(f"Migration duration test failed for {dataset.name}: {e}")
    
    def _test_resource_utilization(self):
        """Test resource utilization during migration operations"""
        
        # Create test datasets of different sizes for resource testing
        test_sizes = [100, 500, 1000, 5000]  # Record counts
        
        for size in test_sizes:
            self.logger.info(f"Testing resource utilization with {size} records")
            
            try:
                test_db = self._create_test_dataset(size)
                
                metrics = self._measure_migration_operation(
                    f"resource_utilization_{size}_records",
                    lambda: self._simulate_migration_process(test_db)
                )
                metrics.records_processed = size
                metrics.throughput_records_per_sec = size / metrics.duration if metrics.duration > 0 else 0
                
                self.performance_metrics.append(metrics)
                
                # Clean up
                test_db.unlink()
                
            except Exception as e:
                self.logger.error(f"Resource utilization test failed for {size} records: {e}")
    
    def _test_scalability(self):
        """Test scalability with increasing data volumes"""
        
        # Progressive scaling test
        scale_sizes = [50, 100, 250, 500, 1000, 2500, 5000, 10000]
        
        for size in scale_sizes:
            self.logger.info(f"Scalability test with {size} records")
            
            try:
                test_db = self._create_test_dataset(size)
                
                # Measure comprehensive metrics
                start_time = time.time()
                memory_before = self.process.memory_info().rss / (1024**2)
                cpu_before = psutil.cpu_percent(interval=0.1)
                
                # Run migration process
                self._simulate_migration_process(test_db)
                
                end_time = time.time()
                processing_time = end_time - start_time
                memory_after = self.process.memory_info().rss / (1024**2)
                cpu_after = psutil.cpu_percent(interval=0.1)
                
                throughput = size / processing_time if processing_time > 0 else 0
                performance_score = self._calculate_performance_score(size, processing_time, memory_after - memory_before)
                
                result = ScalabilityTestResult(
                    dataset_size=size,
                    records_count=size,
                    processing_time=processing_time,
                    memory_usage_mb=memory_after - memory_before,
                    cpu_usage_percent=(cpu_before + cpu_after) / 2,
                    throughput=throughput,
                    performance_score=performance_score
                )
                
                self.scalability_results.append(result)
                
                # Clean up
                test_db.unlink()
                
            except Exception as e:
                self.logger.error(f"Scalability test failed for {size} records: {e}")
    
    def _analyze_bottlenecks(self):
        """Analyze performance bottlenecks"""
        
        self.logger.info("Analyzing performance bottlenecks")
        
        # Analyze scalability results for bottlenecks
        if len(self.scalability_results) >= 2:
            # Check for non-linear performance degradation
            prev_result = None
            for result in self.scalability_results:
                if prev_result:
                    # Check for significant performance drops
                    size_ratio = result.dataset_size / prev_result.dataset_size
                    time_ratio = result.processing_time / prev_result.processing_time
                    
                    if time_ratio > size_ratio * 2:  # More than 2x slower than expected
                        self.bottlenecks.append(f"Performance degradation at {result.dataset_size} records")
                    
                    # Check memory usage
                    memory_ratio = result.memory_usage_mb / max(prev_result.memory_usage_mb, 1)
                    if memory_ratio > size_ratio * 1.5:  # Memory usage growing faster than data
                        self.bottlenecks.append(f"Memory inefficiency at {result.dataset_size} records")
                
                prev_result = result
        
        # Analyze individual operation metrics
        slow_operations = [m for m in self.performance_metrics if m.duration > 5.0]
        if slow_operations:
            self.bottlenecks.append(f"Slow operations detected: {len(slow_operations)} operations took >5 seconds")
        
        memory_intensive = [m for m in self.performance_metrics if m.memory_peak_mb > 100]
        if memory_intensive:
            self.bottlenecks.append(f"Memory intensive operations: {len(memory_intensive)} operations used >100MB")
    
    def _test_memory_efficiency(self):
        """Test memory efficiency during operations"""
        
        self.logger.info("Testing memory efficiency")
        
        # Force garbage collection before test
        gc.collect()
        
        try:
            # Create a moderate-sized dataset for memory testing
            test_db = self._create_test_dataset(1000)
            
            # Test memory usage during different operations
            operations = [
                ("version_detection", lambda: self._test_version_detection_memory(test_db)),
                ("schema_analysis", lambda: self._test_schema_analysis_memory(test_db)),
                ("data_validation", lambda: self._test_data_validation_memory(test_db))
            ]
            
            for op_name, operation in operations:
                metrics = self._measure_migration_operation(f"memory_efficiency_{op_name}", operation)
                self.performance_metrics.append(metrics)
            
            # Clean up
            test_db.unlink()
            
        except Exception as e:
            self.logger.error(f"Memory efficiency test failed: {e}")
    
    def _test_concurrent_operations(self):
        """Test performance under concurrent operations"""
        
        self.logger.info("Testing concurrent operations performance")
        
        try:
            # Create multiple test datasets
            test_datasets = []
            for i in range(3):
                test_db = self._create_test_dataset(500, f"concurrent_test_{i}")
                test_datasets.append(test_db)
            
            # Test concurrent access
            def concurrent_operation(dataset):
                return self._simulate_migration_process(dataset)
            
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = [executor.submit(concurrent_operation, db) for db in test_datasets]
                results = [future.result() for future in futures]
            
            end_time = time.time()
            
            metrics = PerformanceMetrics(
                operation_name="concurrent_operations",
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                cpu_usage_before=0,
                cpu_usage_after=0,
                memory_before_mb=0,
                memory_after_mb=0,
                memory_peak_mb=0,
                disk_read_before=0,
                disk_write_before=0,
                disk_read_after=0,
                disk_write_after=0,
                records_processed=1500,  # 3 * 500
                throughput_records_per_sec=1500 / (end_time - start_time),
                additional_metrics={"concurrent_datasets": 3, "results": len(results)}
            )
            
            self.performance_metrics.append(metrics)
            
            # Clean up
            for db in test_datasets:
                db.unlink()
                
        except Exception as e:
            self.logger.error(f"Concurrent operations test failed: {e}")
    
    def _test_database_benchmarks(self):
        """Run database operation benchmarks"""
        
        self.logger.info("Running database benchmarks")
        
        try:
            # Create test database for benchmarking
            test_db = self._create_test_dataset(1000)
            
            # Test various database operations
            operations = [
                ("select_all", lambda: self._benchmark_select_operations(test_db)),
                ("indexed_queries", lambda: self._benchmark_indexed_queries(test_db)),
                ("aggregation_queries", lambda: self._benchmark_aggregation_queries(test_db))
            ]
            
            for op_name, operation in operations:
                metrics = self._measure_migration_operation(f"db_benchmark_{op_name}", operation)
                self.performance_metrics.append(metrics)
            
            # Clean up
            test_db.unlink()
            
        except Exception as e:
            self.logger.error(f"Database benchmarks failed: {e}")
    
    def _measure_migration_operation(self, operation_name: str, operation_func) -> PerformanceMetrics:
        """Measure performance metrics for a migration operation"""
        
        # Force garbage collection before measurement
        gc.collect()
        
        # Get initial measurements
        start_time = time.time()
        cpu_before = psutil.cpu_percent(interval=0.1)
        memory_before = self.process.memory_info().rss / (1024**2)
        
        try:
            disk_io_before = psutil.disk_io_counters()
            disk_read_before = disk_io_before.read_bytes if disk_io_before else 0
            disk_write_before = disk_io_before.write_bytes if disk_io_before else 0
        except:
            disk_read_before = disk_write_before = 0
        
        # Track peak memory usage
        peak_memory = memory_before
        
        def memory_monitor():
            nonlocal peak_memory
            while hasattr(memory_monitor, 'running'):
                current_memory = self.process.memory_info().rss / (1024**2)
                peak_memory = max(peak_memory, current_memory)
                time.sleep(0.1)
        
        # Start memory monitoring
        memory_monitor.running = True
        monitor_thread = threading.Thread(target=memory_monitor)
        monitor_thread.start()
        
        try:
            # Execute the operation
            result = operation_func()
            
        finally:
            # Stop memory monitoring
            delattr(memory_monitor, 'running')
            monitor_thread.join(timeout=1)
        
        # Get final measurements
        end_time = time.time()
        cpu_after = psutil.cpu_percent(interval=0.1)
        memory_after = self.process.memory_info().rss / (1024**2)
        
        try:
            disk_io_after = psutil.disk_io_counters()
            disk_read_after = disk_io_after.read_bytes if disk_io_after else 0
            disk_write_after = disk_io_after.write_bytes if disk_io_after else 0
        except:
            disk_read_after = disk_write_after = 0
        
        return PerformanceMetrics(
            operation_name=operation_name,
            start_time=start_time,
            end_time=end_time,
            duration=end_time - start_time,
            cpu_usage_before=cpu_before,
            cpu_usage_after=cpu_after,
            memory_before_mb=memory_before,
            memory_after_mb=memory_after,
            memory_peak_mb=peak_memory,
            disk_read_before=disk_read_before,
            disk_write_before=disk_write_before,
            disk_read_after=disk_read_after,
            disk_write_after=disk_write_after,
            records_processed=0,  # Will be set by caller if applicable
            throughput_records_per_sec=0,  # Will be calculated by caller if applicable
            additional_metrics={"result": str(result) if result else "completed"}
        )
    
    def _create_test_dataset(self, record_count: int, name_suffix: str = "") -> Path:
        """Create a test dataset with specified number of records"""
        
        test_name = f"perf_test_{record_count}_{name_suffix}_{int(time.time())}.db"
        test_db = self.results_dir / test_name
        
        conn = sqlite3.connect(str(test_db))
        cursor = conn.cursor()
        
        # Create table
        cursor.execute("""
            CREATE TABLE downloads (
                id INTEGER PRIMARY KEY,
                url TEXT NOT NULL,
                title TEXT,
                description TEXT,
                download_date TEXT,
                file_size INTEGER,
                status TEXT DEFAULT 'pending'
            )
        """)
        
        # Insert test data
        for i in range(record_count):
            cursor.execute("""
                INSERT INTO downloads (url, title, description, download_date, file_size, status)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                f"https://example.com/video{i}",
                f"Test Video {i}",
                f"Description for test video {i}",
                "2024-01-01",
                1024 * (i % 100 + 1),  # Varying file sizes
                "completed" if i % 3 == 0 else "pending"
            ))
        
        conn.commit()
        conn.close()
        
        return test_db
    
    def _simulate_migration_process(self, dataset_path: Path) -> bool:
        """Simulate migration process for performance testing"""
        
        try:
            config = ConnectionConfig(database_path=str(dataset_path))
            connection_manager = SQLiteConnectionManager(config)
            connection_manager.initialize()
            
            try:
                # Simulate version detection
                version_manager = VersionManager(connection_manager)
                version_info = version_manager.get_current_version_info()
                
                # Simulate data integrity check
                integrity_manager = DataIntegrityManager(connection_manager)
                success, message, result = integrity_manager.validate_migration_integrity(ValidationLevel.STANDARD)
                
                return True
                
            finally:
                connection_manager.shutdown()
                
        except Exception as e:
            self.logger.error(f"Migration simulation failed: {e}")
            return False
    
    def _test_version_detection_memory(self, dataset_path: Path) -> bool:
        """Test memory usage during version detection"""
        
        config = ConnectionConfig(database_path=str(dataset_path))
        connection_manager = SQLiteConnectionManager(config)
        connection_manager.initialize()
        
        try:
            version_manager = VersionManager(connection_manager)
            version_info = version_manager.get_current_version_info()
            return True
        finally:
            connection_manager.shutdown()
    
    def _test_schema_analysis_memory(self, dataset_path: Path) -> bool:
        """Test memory usage during schema analysis"""
        
        config = ConnectionConfig(database_path=str(dataset_path))
        connection_manager = SQLiteConnectionManager(config)
        connection_manager.initialize()
        
        try:
            schema_manager = SchemaTransformationManager(connection_manager)
            # Simulate schema analysis operations
            return True
        finally:
            connection_manager.shutdown()
    
    def _test_data_validation_memory(self, dataset_path: Path) -> bool:
        """Test memory usage during data validation"""
        
        config = ConnectionConfig(database_path=str(dataset_path))
        connection_manager = SQLiteConnectionManager(config)
        connection_manager.initialize()
        
        try:
            integrity_manager = DataIntegrityManager(connection_manager)
            success, message, result = integrity_manager.validate_migration_integrity(ValidationLevel.STANDARD)
            return success
        finally:
            connection_manager.shutdown()
    
    def _benchmark_select_operations(self, dataset_path: Path) -> int:
        """Benchmark select operations"""
        
        conn = sqlite3.connect(str(dataset_path))
        cursor = conn.cursor()
        
        # Run various select operations
        cursor.execute("SELECT COUNT(*) FROM downloads")
        count = cursor.fetchone()[0]
        
        cursor.execute("SELECT * FROM downloads LIMIT 100")
        results = cursor.fetchall()
        
        conn.close()
        return len(results)
    
    def _benchmark_indexed_queries(self, dataset_path: Path) -> int:
        """Benchmark indexed queries"""
        
        conn = sqlite3.connect(str(dataset_path))
        cursor = conn.cursor()
        
        # Create index for testing
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_status ON downloads(status)")
        
        # Run indexed queries
        cursor.execute("SELECT * FROM downloads WHERE status = 'completed'")
        results = cursor.fetchall()
        
        conn.close()
        return len(results)
    
    def _benchmark_aggregation_queries(self, dataset_path: Path) -> int:
        """Benchmark aggregation queries"""
        
        conn = sqlite3.connect(str(dataset_path))
        cursor = conn.cursor()
        
        # Run aggregation queries
        cursor.execute("SELECT status, COUNT(*), AVG(file_size) FROM downloads GROUP BY status")
        results = cursor.fetchall()
        
        conn.close()
        return len(results)
    
    def _calculate_performance_score(self, dataset_size: int, processing_time: float, memory_usage: float) -> float:
        """Calculate performance score based on multiple factors"""
        
        # Base score starts at 100
        score = 100.0
        
        # Penalize slow processing (target: 1000 records/second)
        target_throughput = 1000  # records per second
        actual_throughput = dataset_size / processing_time if processing_time > 0 else 0
        throughput_ratio = actual_throughput / target_throughput
        score *= min(throughput_ratio, 1.0)
        
        # Penalize high memory usage (target: <1MB per 1000 records)
        target_memory_per_1000 = 1.0  # MB
        expected_memory = (dataset_size / 1000) * target_memory_per_1000
        if memory_usage > expected_memory:
            memory_penalty = expected_memory / memory_usage
            score *= memory_penalty
        
        return max(score, 0.0)
    
    def _generate_performance_report(self, start_time: datetime) -> PerformanceReport:
        """Generate comprehensive performance report"""
        
        end_time = datetime.now()
        
        # Calculate overall performance score
        if self.scalability_results:
            overall_score = sum(r.performance_score for r in self.scalability_results) / len(self.scalability_results)
        else:
            overall_score = 0.0
        
        # Generate optimization recommendations
        recommendations = self._generate_optimization_recommendations()
        
        report = PerformanceReport(
            session_id=self.session_id,
            test_timestamp=datetime.now().isoformat(),
            total_tests=len(self.performance_metrics),
            performance_metrics=self.performance_metrics,
            scalability_results=self.scalability_results,
            bottlenecks_identified=self.bottlenecks,
            optimization_recommendations=recommendations,
            system_specifications=self.system_specs,
            overall_performance_score=overall_score
        )
        
        # Save report
        self._save_performance_report(report)
        
        return report
    
    def _generate_optimization_recommendations(self) -> List[str]:
        """Generate optimization recommendations based on test results"""
        
        recommendations = []
        
        # Analyze bottlenecks for recommendations
        if any("memory" in bottleneck.lower() for bottleneck in self.bottlenecks):
            recommendations.append("MEMORY: Consider implementing streaming data processing to reduce memory usage")
            recommendations.append("MEMORY: Add memory usage monitoring and automatic garbage collection")
        
        if any("performance degradation" in bottleneck for bottleneck in self.bottlenecks):
            recommendations.append("SCALABILITY: Implement batch processing for large datasets")
            recommendations.append("SCALABILITY: Consider parallel processing for independent operations")
        
        # Analyze slow operations
        slow_ops = [m for m in self.performance_metrics if m.duration > 2.0]
        if slow_ops:
            recommendations.append("PERFORMANCE: Optimize slow operations - consider database indexing")
            recommendations.append("PERFORMANCE: Profile slow operations for specific bottlenecks")
        
        # Analyze scalability results
        if len(self.scalability_results) >= 3:
            # Check if performance scales linearly
            sizes = [r.dataset_size for r in self.scalability_results]
            times = [r.processing_time for r in self.scalability_results]
            
            # Simple linear regression check
            if len(sizes) > 1:
                size_growth = sizes[-1] / sizes[0] if sizes[0] > 0 else 1
                time_growth = times[-1] / times[0] if times[0] > 0 else 1
                
                if time_growth > size_growth * 1.5:
                    recommendations.append("SCALABILITY: Non-linear performance degradation detected - review algorithms")
        
        if not recommendations:
            recommendations.append("EXCELLENT: No significant performance issues detected")
            recommendations.append("MONITORING: Continue monitoring performance as data scales")
        
        return recommendations
    
    def _save_performance_report(self, report: PerformanceReport):
        """Save performance testing report"""
        
        report_file = self.results_dir / f"{report.session_id}_report.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, default=str)
        
        self.logger.info(f"Performance report saved to: {report_file}")
        
        # Create summary
        summary_file = self.results_dir / f"{report.session_id}_summary.txt"
        self._create_summary_report(report, summary_file)
    
    def _create_summary_report(self, report: PerformanceReport, summary_file: Path):
        """Create human-readable summary report"""
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("PERFORMANCE AND SCALABILITY TESTING SUMMARY\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Session ID: {report.session_id}\n")
            f.write(f"Timestamp: {report.test_timestamp}\n")
            f.write(f"Overall Performance Score: {report.overall_performance_score:.1f}/100\n\n")
            
            f.write("SYSTEM SPECIFICATIONS:\n")
            for key, value in report.system_specifications.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            f.write("PERFORMANCE METRICS SUMMARY:\n")
            f.write(f"Total Tests: {report.total_tests}\n")
            
            if report.performance_metrics:
                avg_duration = sum(m.duration for m in report.performance_metrics) / len(report.performance_metrics)
                max_duration = max(m.duration for m in report.performance_metrics)
                avg_memory = sum(m.memory_peak_mb for m in report.performance_metrics) / len(report.performance_metrics)
                
                f.write(f"Average Duration: {avg_duration:.2f}s\n")
                f.write(f"Max Duration: {max_duration:.2f}s\n")
                f.write(f"Average Peak Memory: {avg_memory:.1f}MB\n\n")
            
            if report.scalability_results:
                f.write("SCALABILITY RESULTS:\n")
                f.write("Size (records) | Time (s) | Memory (MB) | Throughput (rec/s) | Score\n")
                f.write("-" * 65 + "\n")
                
                for result in report.scalability_results:
                    f.write(f"{result.dataset_size:13d} | {result.processing_time:8.2f} | "
                           f"{result.memory_usage_mb:11.1f} | {result.throughput:18.1f} | "
                           f"{result.performance_score:5.1f}\n")
                f.write("\n")
            
            if report.bottlenecks_identified:
                f.write("BOTTLENECKS IDENTIFIED:\n")
                for i, bottleneck in enumerate(report.bottlenecks_identified, 1):
                    f.write(f"{i}. {bottleneck}\n")
                f.write("\n")
            
            f.write("OPTIMIZATION RECOMMENDATIONS:\n")
            for i, rec in enumerate(report.optimization_recommendations, 1):
                f.write(f"{i}. {rec}\n")

def main():
    """Main entry point for performance testing"""
    print("Performance and Scalability Testing - Task 14.5")
    print("=" * 60)
    
    framework = PerformanceTestingFramework()
    
    try:
        report = framework.run_comprehensive_performance_testing()
        
        print(f"\n‚úÖ Performance testing completed!")
        print(f"üìä Overall Performance Score: {report.overall_performance_score:.1f}/100")
        print(f"üìà Total Tests: {report.total_tests}")
        print(f"üö® Bottlenecks Identified: {len(report.bottlenecks_identified)}")
        print(f"üìù Report saved in: scripts/performance_results/")
        
        return 0 if report.overall_performance_score >= 70 else 1
        
    except Exception as e:
        print(f"‚ùå Performance testing failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 